\subsection{Principe et motivation}

Contrairement à l'approche précédente où l'agent LRI choisissait directement parmi un ensemble de pentes fixes, nous introduisons ici une approche plus flexible basée sur un \textbf{triplet de probabilités d'actions}. Cette approche transforme le problème de sélection d'action en un problème d'apprentissage de la meilleure distribution de probabilités sur trois actions fondamentales:

$$[p_{\text{diminuer}}, p_{\text{maintenir}}, p_{\text{augmenter}}]$$

Cette méthode offre plusieurs avantages:
\begin{itemize}
    \item \textbf{Adaptabilité accrue aux conditions changeantes}: Une politique probabiliste peut s'ajuster graduellement en fonction des retours de l'environnement, sans passer brusquement d'une stratégie à une autre.
    
    \item \textbf{Exploration plus efficace de l'espace des stratégies}: La sélection d'actions étant probabiliste, cela favorise naturellement l'exploration (surtout en début d'entraînement), sans nécessiter un mécanisme explicite d'exploration comme $\epsilon$-greedy \cite{ThePolicyGradientTheorem}.
    
    \item \textbf{Comportement plus nuancé}: Cette approche permet des ajustements subtils, particulièrement adaptés aux environnements continus ou bruités, où la simple sélection de pentes fixes manquerait de finesse.
\end{itemize}

\subsection{Fonctionnement du modèle}

Dans notre implémentation, nous utilisons le même modèle utilisateur que celui de l'approche par pentes, mais avec deux modifications significatives:

\begin{enumerate}
    \item \textbf{Fonction de récompense additive}: Contrairement à la formulation multiplicative précédente, nous adoptons une combinaison linéaire des deux composantes:
    $$r_{n,n'} = ren_{n,n'} + \gamma \cdot rcs_{n,n'}$$
    Cette formulation permet un meilleur équilibre entre les objectifs et facilite l'interprétation du paramètre $\gamma$.
    
    \item \textbf{Espace d'actions probabiliste}: À chaque pas de temps, l'agent décide stochastiquement s'il doit diminuer, maintenir ou augmenter le signal avec un pas fixe (typiquement 5 unités).
\end{enumerate}

\subsubsection{Algorithme LRI pour triplets de probabilités}

Notre implémentation de l'algorithme LRI pour triplets de probabilités combine la flexibilité stochastique avec un apprentissage adaptatif. Le pseudo-code complet est présenté en annexe, mais voici les principes fondamentaux:

\begin{center}
    \textbf{Algorithme 1 : Linear Reward Inaction (LRI) avec triplets de probabilités}
\end{center}

L'algorithme maintient une distribution de probabilités sur un ensemble de triplets possibles. À chaque étape:
\begin{enumerate}
    \item Un triplet $[p_{\text{diminuer}}, p_{\text{maintenir}}, p_{\text{augmenter}}]$ est sélectionné selon cette distribution
    \item Une action élémentaire (diminuer/maintenir/augmenter) est choisie en fonction des probabilités du triplet sélectionné
    \item Après observation de la réaction utilisateur, une récompense est calculée
    \item La distribution de probabilités sur les triplets est mise à jour, renforçant uniquement ceux qui produisent des récompenses positives
\end{enumerate}

Un mécanisme de gestion des contraintes physiques est également intégré: lorsque le signal atteint sa valeur maximale, la probabilité d'augmentation est temporairement mise à zéro et les autres probabilités sont renormalisées. De façon similaire, à la valeur minimale, la probabilité de diminution est désactivée.

Cette formulation permet à l'agent d'apprendre non seulement quelle action est optimale, mais aussi dans quelles proportions combiner les actions élémentaires pour maximiser la récompense à long terme, créant ainsi un comportement adaptatif finement ajusté.\\

Pour mieux comprendre le fonctionnement de l'algorithme LRI dans notre contexte, examinons en détail un exemple concret du processus d'apprentissage sur un horizon limité de 10 itérations.
\small\itshape

% \textbf{Exemple du processus d'apprentissage LRI sur 10 itérations:}

Avec $\alpha = 0.01$, le tableau suivant illustre l'évolution des probabilités de sélection pour 5 triplets prédéfinis. À l'état initial, chaque triplet a une probabilité identique de 0.2 (distribution uniforme). À chaque itération, un triplet est sélectionné, une action est tirée selon sa distribution de probabilités, et la récompense résultante modifie les probabilités de sélection futures.\\

Le processus de calcul et de mise à jour des probabilités pour l'itération 4 est présenté en annexe, illustrant concrètement l'application de l'algorithme LRI.
\begin{lstlisting}[style=mystyle, caption={Détail de l'itération 4 dans l'algorithme LRI}, escapechar=|]

\end{lstlisting}




\begin{table}[h]
\centering
\scriptsize
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{It.} & \textbf{Triplet} & \textbf{Tirage} & \textbf{Action} & \textbf{T°} & \textbf{Réc.} & \textbf{P(T1)} & \textbf{P(T2)} & \textbf{P(T3)} & \textbf{P(T4)} & \textbf{P(T5)} \\
\hline
0 & - & - & - & 90 & - & .2000 & .2000 & .2000 & .2000 & .2000 \\
\hline
1 & T1[.3,.6,.1] & .423 & Maint. & 90 & 1.000 & .2008 & .1998 & .1998 & .1998 & .1998 \\
\hline
2 & T1[.3,.6,.1] & .189 & Dim. & 85 & 1.056 & .2017 & .1996 & .1996 & .1996 & .1996 \\
\hline
3 & T5[.5,.5,.0] & .412 & Dim. & 80 & 1.111 & .2015 & .1994 & .1994 & .1994 & .2004 \\
\hline
4 & T3[.4,.5,.1] & .755 & Maint. & 80 & 1.111 & .2013 & .1992 & .2004 & .1992 & .2001 \\
\hline
5 & T1[.3,.6,.1] & .378 & Maint. & 80 & 1.111 & .2025 & .1990 & .2002 & .1990 & .1999 \\
\hline
6 & T1[.3,.6,.1] & .881 & Aug. & 85 & 1.056 & .2034 & .1988 & .1999 & .1988 & .1996 \\
\hline
7 & T3[.4,.5,.1] & .243 & Dim. & 80 & 1.111 & .2030 & .1986 & .2011 & .1986 & .1994 \\
\hline
8 & T1[.3,.6,.1] & .165 & Dim. & 75 & 1.167 & .2042 & .1984 & .2008 & .1984 & .1991 \\
\hline
9 & T1[.3,.6,.1] & .622 & Maint. & 75 & 1.167 & .2054 & .1981 & .2005 & .1981 & .1988 \\
\hline
10 & T1[.3,.6,.1] & .257 & Dim. & 70 & 1.222 & .2069 & .1978 & .2001 & .1978 & .1984 \\
\hline
\end{tabular}
\caption{Évolution des probabilités de sélection des triplets sur 10 itérations}
\label{tab:exemple_apprentissage}
\end{table}

Même sur ce court exemple démonstratif, plusieurs tendances significatives peuvent être observées:

\begin{itemize}
    \item \textbf{Émergence d'un triplet dominant}: Le triplet T1 a été sélectionné 6 fois sur 10 et sa probabilité a augmenté de manière significative (de 0.2000 à 0.2069, soit +0.0069)
    
    \item \textbf{Différenciation progressive}: Les triplets non sélectionnés (T2 et T4) voient leurs probabilités diminuer le plus rapidement
    
    \item \textbf{Corrélation récompense-sélection}: Les récompenses plus élevées obtenues aux basses températures (jusqu'à 1.2222 pour 70°) favorisent logiquement les triplets permettant cette diminution
    
    \item \textbf{Adaptation dynamique}: La distribution de probabilités d'actions change parfois en fonction des contraintes (lignes 1-2, absence d'augmentation possible à la température maximale)
\end{itemize}

Cet exemple illustre l'amorce du processus de convergence où le triplet T1, avec sa combinaison équilibrée de 30\% de diminution, 60\% de maintien et 10\% d'augmentation, commence à émerger comme stratégie dominante. Sur le long terme (millions d'itérations de nos expériences complètes), ce mécanisme permet une convergence vers les triplets réellement optimaux pour l'environnement et les préférences spécifiées.

La force de l'algorithme LRI réside dans cette capacité à identifier progressivement les meilleures stratégies sans connaissance préalable de l'environnement, uniquement guidé par les récompenses obtenues au fil des interactions.
\normalfont\normalsize
\normalsize


\subsection{Expériences et résultats \cite{LRIprobabiliste}}

\subsubsection{Expérience 1: Exploration systématique de l'espace des probabilités}

La première expérience réalise une exploration exhaustive de l'espace des triplets de probabilités pour déterminer leur impact sur:
\begin{itemize}
    \item La consommation énergétique
    \item Le paramètre $m$ reflétant le confort utilisateur
\end{itemize}

\paragraph{Méthodologie d'exploration}
\begin{itemize}
    \item \textbf{Création d'une grille de probabilités}: 
    Les probabilités sont échantillonnées sur une grille 2D où les axes représentent $p_{\text{diminuer}}$ et $p_{\text{augmenter}}$, avec $p_{\text{maintenir}} = 1 - p_{\text{diminuer}} - p_{\text{augmenter}}$.
    
    \item \textbf{Filtrage des combinaisons valides}: 
    Seules les combinaisons où $p_{\text{diminuer}} + p_{\text{augmenter}} \leq 1$ (pour que $p_{\text{maintenir}} \geq 0$) sont conservées.
    
    \item \textbf{Échantillonnage systématique}: 
    L'espace des probabilités est échantillonné avec une granularité de 0.05, générant 231 triplets valides.
    
    \item \textbf{Test statistique}: 
    Chaque triplet valide est testé sur 50 exécutions indépendantes de 10000 pas pour obtenir des résultats statistiquement significatifs.
\end{itemize}

\paragraph{Déroulement typique pour un triplet}
Prenons l'exemple du triplet $[0.3, 0.6, 0.1]$ ($p_{\text{diminuer}}=0.3$, $p_{\text{maintenir}}=0.6$, $p_{\text{augmenter}}=0.1$):

\begin{enumerate}
    \item \textbf{Initialisation}:
    \begin{itemize}
        \item Paramètres initiaux du modèle utilisateur ($a_0=0.2$, $m_0=35$, etc.)
        \item Signal initial à sa valeur maximale (90)
    \end{itemize}
    
    \item \textbf{À chaque pas de simulation}:
    \begin{itemize}
        \item Une action est sélectionnée selon les probabilités du triplet
        \item Le signal est modifié selon l'action ($\pm$ 5 unités ou maintien)
        \item Le modèle utilisateur met à jour ses paramètres internes
        \item Si l'utilisateur n'intervient pas → on continue avec la nouvelle valeur du signal
        \item Si l'utilisateur intervient → retour à la valeur maximale du signal
    \end{itemize}
    
    \item \textbf{Mesures calculées}:
    \begin{itemize}
        \item Énergie moyenne consommée (moyenne des valeurs du signal)
        \item Valeur moyenne du paramètre $m$ (seuil d'intervention)
        \item Nombre d'interventions utilisateur
    \end{itemize}
\end{enumerate}

Cette approche systématique permet de cartographier l'ensemble de l'espace des stratégies probabilistes et d'identifier les zones optimales en fonction de différents critères.

% \paragraph{Résultats principaux}
% Les résultats démontrent une structure claire dans l'espace des triplets:
% \begin{itemize}
%     \item Les triplets avec une forte probabilité de diminution et une faible probabilité d'augmentation minimisent la consommation énergétique
%     \item Les triplets avec une faible probabilité de diminution et une forte probabilité de maintien maximisent le confort (paramètre $m$ bas)
% \end{itemize}

% L'analyse révèle deux configurations optimales distinctes:
% \begin{mdframed}
% \textbf{Configuration optimale pour l'énergie minimale:}\\
% Triplet: [0.75, 0.00, 0.25]\\
% Énergie moyenne: 64.30\\
% Paramètre $m$: 36.45 \\
% Nombre d'interventions: 255.43


% \textbf{Configuration optimale pour le confort (minimisation de $m$):}\\
% Triplet: [0.00, 1.00, 0.00]\\
% Énergie moyenne: 65.01\\
% Paramètre $m$: 37.88 \\
% Nombre d'interventions: 454.00
% \end{mdframed}

% \begin{figure}[h]
%     \centering
%     \caption{Distribution de l'énergie moyenne en fonction des triplets de probabilités}
%     \label{fig:energie_3d}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \caption{Distribution du paramètre $m$ en fonction des triplets de probabilités}
%     \label{fig:parametre_m_3d}
% \end{figure}
% À l'issue de cette exploration, la structure de l'espace des probabilités révèle des configurations optimales distinctes. D'une part, le triplet [0.00, 1.00, 0.00] maximise le confort utilisateur en maintenant constamment le signal à sa valeur maximale, ce qui évite toute intervention de l'utilisateur. Cette stratégie, bien que parfaite pour le confort, n'offre cependant aucune possibilité d'optimisation énergétique. D'autre part, le triplet [0.60, 0.40, 0.00] représente le meilleur équilibre pour l'efficacité énergétique, permettant une réduction significative de la consommation tout en maintenant un confort utilisateur acceptable, comme en témoigne la valeur modérée du paramètre $m$ (42.58).
\paragraph{Résultats principaux} \

Les résultats révèlent une structure claire dans l'espace des triplets de probabilités, avec deux tendances opposées:

\begin{itemize}
    \item Les triplets privilégiant une probabilité élevée de diminution ($p_{dim}$) et faible d'augmentation optimisent la consommation d'énergie
    \item Les triplets privilégiant le maintien du signal avec une probabilité faible de diminution minimisent les interventions utilisateur
\end{itemize}

L'analyse identifie deux configurations représentatives du spectre efficacité-confort:

\begin{mdframed}
\textbf{Configuration optimale pour l'efficacité énergétique:}\\
Triplet: [0.75, 0.00, 0.25]\\
Énergie moyenne: 64.30\\
Paramètre $m$: 36.45 \\
Nombre d'interventions: 255.43

\textbf{Configuration sans intervention utilisateur:}\\
Triplet: [0.00, 0.60-0.90, 0.10-0.40]\\
Énergie moyenne: 83.50-90.00\\
Paramètre $m$: 35.00 \\
Nombre d'interventions: 0.00
\end{mdframed}

\begin{figure}[h]
    \centering
    \caption{Distribution de l'énergie moyenne en fonction des triplets de probabilités}
    \label{fig:energie_3d}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Distribution du paramètre $m$ en fonction des triplets de probabilités}
    \label{fig:parametre_m_3d}
\end{figure}
Notre analyse montre que les configurations avec absence totale d'intervention (triplets [0.00, 0.60-0.90, 0.10-0.40]) représentent l'optimum de confort, maintenant le signal à des niveaux élevés mais sans économie d'énergie significative. De l'autre côté, le triplet [0.75, 0.00, 0.25] offre le meilleur compromis pour l'efficacité énergétique, permettant une réduction significative de la consommation. Pour résoudre notre problématique de recherche, nous devons donc trouver un équilibre entre ces deux extrêmes.
\subsubsection{Expérience 2: Évaluation d'une récompense combinée}

La deuxième expérience adopte une perspective plus globale en considérant une fonction de récompense qui intègre à la fois l'efficacité énergétique et le confort utilisateur.

\paragraph{Méthodologie}
\begin{itemize}
    \item Même exploration systématique, mais avec une fonction de récompense combinée:
    $$\text{récompense} = \text{efficacité\_énergétique} + \gamma \times \text{ratio\_confort}$$
    \item Division de la simulation en épisodes, chacun se terminant par une intervention utilisateur
    \item Paramètre $\gamma=1$ pour un équilibre initial entre énergie et confort
\end{itemize}

\paragraph{Résultats principaux}\

L'analyse de la récompense combinée identifie un triplet optimal différent:
\begin{mdframed}
\textbf{Meilleur triplet:} [0.10, 0.90, 0.00]\\
\textbf{Récompense maximale:} 1.8241\\
\textbf{Énergie moyenne:} 65.62\\
\textbf{Valeur $m$ moyenne:} 35.14
\end{mdframed}

Ce résultat est particulièrement intéressant car il favorise une approche prudente:
\begin{itemize}
    \item Une faible probabilité de diminution (10\%)
    \item Une forte probabilité de maintien (90\%)
    \item Une probabilité nulle d'augmentation
\end{itemize}

\begin{figure}[h]
    \centering
    \caption{Distribution de la récompense en fonction des triplets de probabilités}
    \label{fig:recompense_3d}
\end{figure}

\subsubsection{Expérience 3: Apprentissage par renforcement de triplets optimaux}

La troisième expérience évalue la capacité du modèle LRI à découvrir automatiquement des triplets optimaux à travers l'apprentissage par renforcement.


\paragraph{Méthodologie}
\begin{itemize}
    \item Initialisation de la distribution de probabilité uniforme sur tous les triplets possibles
    \item Simulation de 1 000 000 pas avec apprentissage LRI continu
    \item Valeur de alpha (Learning Rate) fixée à 0.0005
    \item Répétition sur 50 exécutions indépendantes pour estimer la robustesse
    \item Analyse de la convergence en fonction du paramètre $\gamma$
\end{itemize}


\paragraph{Résultats principaux de l'expérience 3}\

L'analyse des 50 exécutions indépendantes avec $\gamma=1$ (équilibre entre énergie et confort) révèle plusieurs phénomènes d'adaptation:

\begin{enumerate}
    \item \textbf{Évolution de la consommation d'énergie}:
    \begin{itemize}
        \item Phase initiale: Forte diminution rapide de la consommation (de $\sim$69 à $\sim$65.5 unités)
        \item Phase de stabilisation: Oscillations contrôlées autour de 65.5-65.9
        \item Valeur finale moyenne: 65.87 unités d'énergie
    \end{itemize}
    
    \item \textbf{Évolution du seuil d'intervention ($m$)}:
    \begin{itemize}
        \item Phase d'exploration: Augmentation initiale jusqu'à $\sim$35.5 (périodes 10-15)
        \item Phase d'adaptation: Diminution progressive et régulière
        \item Phase de stabilisation: Convergence vers 35.20 avec réduction significative de la variance entre exécutions
    \end{itemize}
    
    \item \textbf{Dynamique des interventions utilisateur}:
    \begin{itemize}
        \item Phase d'apprentissage: Pic d'interventions ($\sim$180) vers les périodes 15-20
        \item Amélioration continue: Réduction régulière et soutenue du nombre d'interventions
        \item Phase mature: Stabilisation autour de 70-80 interventions par période, soit une réduction de plus de 60\% 
    \end{itemize}
    
    \item \textbf{Convergence des triplets de probabilités}:
    \begin{itemize}
        \item Triplet moyen optimal: [0.09, 0.86, 0.05]
        \item Distribution remarquablement stable: 9\% pour diminuer, 86\% pour maintenir, 5\% pour augmenter
        \item Forte consistance entre les différentes exécutions, démontrant la robustesse du processus d'apprentissage
    \end{itemize}
\end{enumerate}

L'agent LRI converge donc vers un comportement majoritairement conservateur (86\% de maintien), avec des ajustements mineurs à la hausse ou à la baisse selon les conditions. Cette stratégie permet de réduire considérablement les interventions utilisateur tout en maintenant une efficacité énergétique élevée.

\begin{figure}[h]
    \centering
    \caption{Évolution des performances du système au cours de l'apprentissage: (a) Consommation d'énergie, (b) Seuil d'intervention $m$, (c) Nombre d'interventions utilisateur, (d) Triplets de probabilités optimaux}
    \label{fig:evolution_triplets}
\end{figure}

Cette convergence vers le triplet [0.09, 0.86, 0.05] confirme la capacité de l'algorithme LRI à découvrir efficacement des politiques optimales dans cet espace de recherche complexe. Il est intéressant de noter que ce résultat est relativement proche du triplet optimal [0.10, 0.90, 0.00] identifié dans l'expérience 2, mais intègre une légère probabilité d'augmentation (5\%) qui améliore l'adaptation aux variations des conditions d'utilisation.
%%%%%%%%%%
\subsubsection{Expérience 4 : Impact de gamma et pre sur les stratégies optimales}

Comme dans l'approche par pentes, et pour explorer plus en profondeur les capacités adaptatives de notre approche par triplet de probabilités, nous avons conduit une série d'expériences visant à étudier l'interaction entre deux paramètres clés:

$\gamma$: coefficient de pondération entre économie d'énergie et confort utilisateur\\
$pre$: facteur de mémorisation de l'utilisateur (plus $pre$ est faible, plus la mémoire est longue)

\paragraph{Méthodologie}\

Nous avons systématiquement exploré une grille de configurations combinant: 
\begin{itemize} 
    \item Trois valeurs de $\gamma$ (0.1, 1.0, 2.0) représentant différentes priorités (forte économie d'énergie, équilibre, confort utilisateur) 
    \item Trois valeurs de $pre$ (0.05, 0.35, 0.95) correspondant à différents profils utilisateurs (mémoire forte, modérée, réactive) 
\end{itemize}

Pour chaque combinaison, nous avons exécuté l'algorithme LRI sur 50 simulations indépendantes de 1 000 000 pas, en analysant la convergence vers des triplets de probabilités optimaux.

\paragraph{Triplets optimaux par configuration}

L'analyse révèle un pattern intéressant d'adaptation du système aux différentes conditions:

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Triplet optimal} & \textbf{Énergie} & \textbf{Seuil $m$} \\
\hline
$\gamma$=0.1, $pre$=0.05 & [0.41, 0.58, 0.01] & 64.83 & 37.09 \\
\hline
$\gamma$=0.1, $pre$=0.35 & [0.44, 0.56, 0.00] & 64.88 & 36.21 \\
\hline
$\gamma$=0.1, $pre$=0.95 & [0.50, 0.49, 0.01] & 64.82 & 36.21 \\
\hline
$\gamma$=1.0, $pre$=0.05 & [0.10, 0.87, 0.04] & 65.64 & 35.25 \\
\hline
$\gamma$=1.0, $pre$=0.35 & [0.10, 0.86, 0.05] & 65.71 & 35.23 \\
\hline
$\gamma$=1.0, $pre$=0.95 & [0.09, 0.87, 0.04] & 65.57 & 35.21 \\
\hline
$\gamma$=2.0, $pre$=0.05 & [0.12, 0.81, 0.07] & 67.70 & 35.20 \\
\hline
$\gamma$=2.0, $pre$=0.35 & [0.10, 0.83, 0.07] & 67.42 & 35.18 \\
\hline
$\gamma$=2.0, $pre$=0.95 & [0.09, 0.85, 0.06] & 67.34 & 35.17 \\
\hline
\end{tabular}
\caption{Triplets optimaux par configuration de $\gamma$ et $pre$}
\label{tab:triplets_gamma_pre}
\end{table}

\paragraph{Impact du paramètre gamma sur les stratégies}\

En agrégeant les résultats par valeur de $\gamma$, nous observons des tendances claires:

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Gamma} & \textbf{p(dim.)} & \textbf{p(maint.)} & \textbf{p(aug.)} & \textbf{Énergie} & \textbf{Priorité} \\
\hline
0.1 & 0.45 & 0.54 & 0.01 & 64.84 & Forte économie d'énergie \\
\hline
1.0 & 0.10 & 0.86 & 0.04 & 65.64 & Équilibre \\
\hline
2.0 & 0.10 & 0.83 & 0.07 & 67.49 & Confort utilisateur \\
\hline
\end{tabular}
\caption{Impact du paramètre $\gamma$ sur les stratégies (moyennes)}
\label{tab:impact_gamma}
\end{table}

L'augmentation de $\gamma$ entraîne: 
\begin{itemize} 
    \item Une réduction drastique de la probabilité de diminution (de 45\% à 10\%) quand on passe d'une priorité forte sur l'économie d'énergie à un équilibre
    \item Une hausse significative de la probabilité d'augmentation (de 1\% à 7\%), multipliant par 7 sa valeur entre les configurations extrêmes
    \item Une augmentation marquée de la probabilité de maintien (de 54\% à 83\%), favorisant la stabilité quand le confort devient prioritaire
    \item Une consommation d'énergie croissante (+4.1\% entre $\gamma$=0.1 et $\gamma$=2.0), reflétant logiquement la priorité accordée au confort
\end{itemize}

\paragraph{Impact du paramètre pre sur les stratégies}

Le facteur de mémorisation utilisateur influence également les stratégies optimales:

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Pre (Mémoire)} & \textbf{p(dim.)} & \textbf{p(maint.)} & \textbf{p(aug.)} & \textbf{Caractéristique} \\
\hline
0.05 (Forte) & 0.21 & 0.75 & 0.04 & Actions plus prudentes \\
\hline
0.35 (Modérée) & 0.21 & 0.75 & 0.04 & Similaire à mémoire forte \\
\hline
0.95 (Réactive) & 0.23 & 0.74 & 0.04 & Légèrement plus réactive \\
\hline
\end{tabular}
\caption{Impact du paramètre $pre$ sur les stratégies (moyennes)}
\label{tab:impact_pre}
\end{table}

Nous observons que: 
\begin{itemize} 
    \item L'impact du paramètre $pre$ est plus subtil que celui de $\gamma$, avec des différences moins marquées entre les profils utilisateurs
    \item Les utilisateurs à mémoire réactive (pre=0.95) sont légèrement plus enclins à l'action de diminution, particulièrement visible pour $\gamma$=0.1 où la probabilité monte jusqu'à 50\%
    \item La probabilité d'augmentation reste stable quelle que soit la mémoire utilisateur, suggérant que cette action est davantage pilotée par le paramètre $\gamma$
    \item Les trois types d'utilisateurs convergent vers des stratégies très similaires pour $\gamma$=1.0 et $\gamma$=2.0
\end{itemize}

\paragraph{Compromis optimaux identifiés}

Cette étude a permis d'identifier plusieurs configurations optimales selon les objectifs prioritaires:

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Objectif} & \textbf{Triplet optimal} & \textbf{Configuration} & \textbf{Performance} \\
\hline
Économie max. & [0.50, 0.49, 0.01] & $\gamma$=0.1, $pre$=0.95 & Énergie: 64.82, Équilibré \\
\hline
Équilibre & [0.09, 0.87, 0.04] & $\gamma$=1.0, $pre$=0.95 & É: 65.57, Stable \\
\hline
Confort optimal & [0.09, 0.85, 0.06] & $\gamma$=2.0, $pre$=0.95 & É: 67.34, Réactivité optimale \\
\hline
\end{tabular}
\caption{Compromis optimaux identifiés}
\label{tab:compromis_optimaux}
\end{table}

\paragraph{Analyse des résultats clés}

Plusieurs observations importantes émergent de ces expériences:

\begin{enumerate} 
    \item \textbf{Effet seuil du paramètre $\gamma$}: Le passage de $\gamma$=0.1 à $\gamma$=1.0 provoque une transition radicale dans la stratégie optimale, avec une inversion des proportions entre diminution (45\% $\rightarrow$ 10\%) et maintien (54\% $\rightarrow$ 86\%)
    
    \item \textbf{Stabilité remarquable du seuil $m$} pour $\gamma \geq 1.0$ (autour de 35.2), démontrant la robustesse du système à maintenir un niveau de confort acceptable malgré des variations de stratégie
    
    \item \textbf{Cohérence des stratégies entre profils utilisateurs} pour les mêmes valeurs de $\gamma$, suggérant que le coefficient de pondération est le facteur déterminant principal
    
    \item \textbf{Émergence de deux régimes distincts}: un régime "économie d'énergie active" ($\gamma$=0.1) caractérisé par une action fréquente de diminution, et un régime "confort stabilisé" ($\gamma \geq 1.0$) dominé par l'action de maintien
    
    \item \textbf{Performance légèrement supérieure des utilisateurs réactifs} (pre=0.95) pour chaque niveau de $\gamma$, contredisant l'intuition initiale qui aurait pu favoriser les utilisateurs à mémoire forte
\end{enumerate}

Ces résultats démontrent la capacité remarquable du système LRI à adapter finement sa stratégie selon deux dimensions complémentaires: 
\begin{itemize} 
    \item Les priorités système (économie vs confort, via $\gamma$), avec un effet de seuil marqué entre $\gamma$=0.1 et $\gamma$=1.0
    \item Les caractéristiques des utilisateurs (capacité de mémorisation, via $pre$), avec un impact plus subtil mais consistant sur les stratégies optimales
\end{itemize}
\subsection{Avantages et limites de l'approche probabiliste}

L'approche par triplet de probabilités présente plusieurs avantages par rapport à la sélection directe de pentes:

\begin{itemize}
    \item \textbf{Flexibilité accrue}: Adaptation dynamique à l'évolution des conditions et préférences utilisateur
    \item \textbf{Comportement plus nuancé}: Mélange stochastique d'actions qui évite les changements trop brusques
    \item \textbf{Paramétrage simple}: Le paramètre $\gamma$ permet d'ajuster facilement le compromis énergie-confort
\end{itemize}

Dans toutes les configurations, l'action "maintenir" reste dominante ($\geq$80\%), reflétant une approche prudente qui évite les variations trop fréquentes du signal. Cette caractéristique est particulièrement adaptée aux systèmes thermiques où la stabilité est préférable.

Le modèle LRI avec triplets de probabilités constitue donc une évolution pertinente de l'approche initiale, offrant un contrôle plus fin et une adaptabilité accrue aux préférences utilisateur.

\section{Conclusion et perspectives}

Ce travail de recherche a exploré deux approches distinctes pour le contrôle adaptatif de signaux environnementaux basées sur l'algorithme d'apprentissage par renforcement LRI: l'approche par pentes fixes et l'approche par triplet de probabilités. La comparaison de ces deux méthodes révèle des différences fondamentales dans leur fonctionnement et leurs performances.

\subsection{Comparaison des approches proposées}

\subsubsection{Différences conceptuelles fondamentales}

\begin{itemize}
    \item \textbf{Approche par pentes}: L'agent choisit directement parmi un ensemble prédéfini de pentes négatives qui dictent la vitesse de diminution du signal. L'espace d'action est discret et limité aux pentes préalablement définies (21 pentes dans notre implémentation).
    
    \item \textbf{Approche par triplet de probabilités}: L'agent apprend une distribution de probabilités sur trois actions fondamentales (diminuer, maintenir, augmenter), créant ainsi un comportement stochastique. L'espace d'action devient continu à travers les distributions de probabilités possibles.
\end{itemize}

\subsubsection{Avantages comparatifs}

\paragraph{Avantages de l'approche par pentes}
\begin{itemize}
    \item \textbf{Simplicité conceptuelle}: Plus facile à comprendre et à interpréter (une pente = une vitesse de diminution)
    \item \textbf{Stabilité comportementale}: Comportement plus prédictible et moins aléatoire
    \item \textbf{Maturité}: Approche validée par l'article original avec des résultats robustes
    \item \textbf{Convergence}: Converge généralement vers une ou deux pentes dominantes (-10 dans notre cas)
\end{itemize}

\paragraph{Avantages de l'approche par triplet de probabilités}
\begin{itemize}
    \item \textbf{Flexibilité accrue}: Adaptation plus fine aux différentes conditions d'utilisation
    \item \textbf{Exploration enrichie}: Couvre un espace d'actions continu plutôt que discret
    \item \textbf{Comportement nuancé}: Génère un équilibre dynamique entre les trois actions fondamentales
    \item \textbf{Représentation intuitive}: Le triplet de probabilités offre une interprétation directe du comportement du système
    \item \textbf{Stabilité thermique}: La probabilité dominante de maintien (≥80\%) favorise naturellement la stabilité du signal
\end{itemize}

\subsubsection{Comparaison des performances}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Critère} & \textbf{Approche par pentes} & \textbf{Approche par triplet} \\
\hline
Convergence avec $\gamma=1$ & Pente -10 dominante & [0.09, 0.86, 0.05] \\
\hline
Consommation énergétique & $\approx$62 unités & 65.87 unités \\
\hline
Confort utilisateur & Variations prononcées & Stabilité accrue (86\% maintien) \\
\hline
Réduction des interventions & $\approx$50\% & $>$60\% \\
\hline
Stabilité thermique & Oscillations visibles & Comportement majoritairement conservateur \\
\hline
Adaptation dynamique & Limitée (choix discret) & Élevée (ajustements probabilistes fins) \\
\hline
\end{tabular}
\caption{Comparaison des performances dans l'Expérience 3}
\label{tab:comparaison_performances_exp3}
\end{table}

L'approche par triplet de probabilités démontre une capacité supérieure à équilibrer l'économie d'énergie et le confort utilisateur. Cette efficacité accrue s'explique par la plus grande flexibilité du modèle, capable de combiner dynamiquement les trois actions fondamentales.

L'analyse des résultats de l'Expérience 3 confirme cette tendance, avec la convergence de l'algorithme LRI vers un triplet [0.09, 0.86, 0.05] lorsque $\gamma=1$. Cette distribution de probabilités, avec une forte prépondérance du maintien (86\%), permet une réduction de plus de 60\% des interventions utilisateur tout en maintenant une consommation énergétique optimisée à 65.87 unités. Ce comportement majoritairement conservateur, complété par des ajustements mineurs (9\% de diminution, 5\% d'augmentation), offre un équilibre remarquable entre stabilité et réactivité que l'approche par pentes ne peut égaler avec ses contraintes discrètes.
\subsubsection{Sensibilité au paramètre $\gamma$}

Les deux approches montrent une sensibilité au paramètre $\gamma$ qui équilibre économie d'énergie et confort:

\begin{itemize}
    \item \textbf{Approche par pentes}: Influence principalement la pente optimale sélectionnée
    \item \textbf{Approche par triplet}: Modifie la distribution des probabilités, avec un impact marqué sur la probabilité d'augmentation (de 1\% à 8\% quand $\gamma$ passe de 0.3 à 1.5)
\end{itemize}

Cette sensibilité différenciée offre deux leviers distincts de personnalisation: sélection d'une courbe de décroissance contre combinaison stochastique d'actions élémentaires.