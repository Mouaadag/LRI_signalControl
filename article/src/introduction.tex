
L'optimisation énergétique des bâtiments constitue un défi environnemental majeur qui nécessite un équilibre entre économies d'énergie et confort utilisateur. Les systèmes de contrôle traditionnels, basés sur des règles statiques et des modèles prédéfinis, montrent leurs limites face à la nature évolutive et non-stationnaire du comportement humain. Ces approches conventionnelles ne parviennent pas à s'adapter efficacement aux préférences individuelles qui changent au fil du temps et des interactions, compromettant ainsi l'efficacité énergétique ou le confort selon les situations.
Ce travail explore une approche innovante pour résoudre ce problème: l'utilisation d'un algorithme d'apprentissage par renforcement sans état, Linear Reward Inaction (LRI), pour développer un système de contrôle adaptatif qui apprend progressivement à équilibrer la consommation énergétique et le confort utilisateur. Deux variantes de cette approche sont proposées et comparées:
\begin{itemize}
    \item Une première variante basée sur la sélection adaptative de pentes de diminution du signal environnemental, reproduisant les résultats de l'article de référence \cite{haddam2022}
    \item Une seconde variante plus flexible que nous proposons, utilisant un triplet de probabilités d'actions élémentaires (diminuer, maintenir, augmenter)
\end{itemize}

Cette étude comprend la formulation mathématique du problème, la modélisation du comportement utilisateur, la conception de l'algorithme d'apprentissage, ainsi qu'une série d'expérimentations numériques permettant d'évaluer les performances et la robustesse des approches proposées. Les résultats démontrent le potentiel de l'apprentissage par renforcement pour créer des systèmes de contrôle qui s'adaptent dynamiquement aux préférences individuelles des utilisateurs tout en optimisant la consommation d'énergie.

Le rapport est structuré comme suit: après cette introduction, nous présentons l'analyse de l'article original, puis les résultats de notre reproduction des expériences. Ensuite, nous détaillons notre extension avec le modèle LRI à triplet de probabilités avant de conclure avec une analyse comparative et des perspectives d'amélioration.
